{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Train a GPT-2 Text-Generating Model w/ GPUex2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rittermi/computationalmedia/blob/main/Copy_of_Train_a_GPT_2_Text_Generating_Model_w_GPUex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c783c7-bdfd-4e3f-a81d-4a3b032e10bd"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21ebb7d-a6ef-44a4-fa8d-fb296cce8f10"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb  3 01:04:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f012f5e-74a1-447e-b08b-62d45c1b86a8"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 387Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 104Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 342Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 195Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 238Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 159Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 156Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebdd638-0967-4a7f-d580-82e8c090a151"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "file_name = \"1322-0.txt\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9460d4-876b-4d1e-dd6f-e6aa6adc7399"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=500,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=100,\n",
        "              save_every=200\n",
        "              )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 213412 tokens\n",
            "Training...\n",
            "[10 | 29.19] loss=3.57 avg=3.57\n",
            "[20 | 51.62] loss=3.53 avg=3.55\n",
            "[30 | 74.72] loss=3.38 avg=3.49\n",
            "[40 | 98.96] loss=3.24 avg=3.43\n",
            "[50 | 123.24] loss=3.19 avg=3.38\n",
            "[60 | 146.85] loss=3.03 avg=3.32\n",
            "[70 | 170.72] loss=3.05 avg=3.28\n",
            "[80 | 194.89] loss=3.07 avg=3.25\n",
            "[90 | 218.84] loss=2.93 avg=3.21\n",
            "[100 | 242.72] loss=2.92 avg=3.18\n",
            "======== SAMPLE 1 ========\n",
            " the and the.\n",
            "  Of course I could not be so proud as to tell in a book the truth of the general state of the Union and the best of it.\n",
            "  This, it would seem, was to be the law of the day, to be duly enacted and duly ratified as by Congress,\n",
            "  What is it, I ask, that is so important?\n",
            "  I would have to know every single thing, all governments are just as important as they are given\n",
            "      to be right or wrong.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "BOOK IV\n",
            "\n",
            "The Passing Years\n",
            "\n",
            "  When we say to the past that we are not yet content with mere years or even\n",
            "      centuries, or eras, we are quite wrong;\n",
            "  We must, of course, be prepared and prepared to go on living longer or die sooner or\n",
            "     \n",
            "  Of a hundred and twenty years or half.\n",
            "\n",
            "  A hundred and twenty years the old days and realities of life will be gone,\n",
            "  And, following them, what is to be new must be, as soon as is found\n",
            "      sufficient to itself.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "BOOK V\n",
            " The Day-Lifts of Democracy\n",
            "\n",
            "  A day-lifts and the night-lights of freedom--in them!\n",
            "  In them! as in all great inventions, cities, factories, farms,\n",
            "      armies and ships, governments,\n",
            "  In them--and henceforth, all for this day will arise and be like\n",
            "      it ever has.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "From Revolutionary Times\n",
            "\n",
            "  From the Revolutionary Times.\n",
            "\n",
            "  There is no life lacking--nothing of time or any man or woman\n",
            "      of beauty, strength, ambition,\n",
            "  Nor in his time, age, or birth, of any other--yet the most perfect\n",
            "      life of him or her.\n",
            "\n",
            "       1776\n",
            "\n",
            "  There is no man in the world who does not say so,\n",
            "  But he or she who feels himself at the cross of these centuries loves America.\n",
            "\n",
            "  A few brief moments of time,\n",
            "      all the times, with America! to-day and to-morrow,\n",
            "  And the States to come, with America on their backs;\n",
            "  What they are and what they could be--all history has taught us.\n",
            "\n",
            "  A few words to our great friends in Europe--one must be ready to\n",
            "      be told with amazement some of the old and new States;\n",
            "  And a few to the States and the Statesmen of them.\n",
            "\n",
            "  No man without his eyesight and manners does not remember America,\n",
            "  The States have so many faces and places,\n",
            "  I myself do not mean them but they are all in America,\n",
            "  They have their own identities and languages,\n",
            "      themselves and the Statesman and the Stateswoman may be in the States and they\n",
            "      are their own Statesmen and Stateswomen.\n",
            "\n",
            "  I have no objection to these Statesmen and Stateswomen in America,\n",
            "  I never think one of them is more worthy here than the Congress.\n",
            "\n",
            "  And a single look and look on history may lead\n",
            "      to be most partial and to help at last.\n",
            "\n",
            "  My countrymen and women are not only worthy to me, they are\n",
            "      indispensable to every one I know.\n",
            "\n",
            "       1885\n",
            "\n",
            "       1871\n",
            "\n",
            "  I salute your greatness--see, you too, America!\n",
            "  O I know what it is to be surrounded by Statesmen and Stateswomen,\n",
            "  I too possess the States with good will and have no right to be jealous.\n",
            "  If any one objectifies me he too should be treated as a friend,\n",
            "  I feel no affection to any man or woman whose views I do not wish to\n",
            "      think.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "BOOK VIII\n",
            "\n",
            "The Man Who Was the Man Who Was the Man Without a Job\n",
            "\n",
            "  With his beard and beard,\n",
            "  And his blood and beard,\n",
            "  Having prepared himself for the present in every way, and preparing himself\n",
            "      for the future,\n",
            "  After an occupation on land, in the sea, and in every form,\n",
            "  And, after finishing his occupation, with his or her belongings,\n",
            "      making his meals, and drinking his wine,\n",
            "  In all his explorations, in the woods and on the prairies,\n",
            "  In the open and open air, in the woods and on the prairies,\n",
            "  In his explorations, in his walks, in the shops and in his houses,\n",
            "  In every business in the state, and he owns and makes his business,\n",
            "\n",
            "[110 | 278.07] loss=2.78 avg=3.15\n",
            "[120 | 302.18] loss=2.59 avg=3.10\n",
            "[130 | 326.28] loss=2.93 avg=3.08\n",
            "[140 | 350.39] loss=2.70 avg=3.05\n",
            "[150 | 374.39] loss=2.87 avg=3.04\n",
            "[160 | 398.31] loss=2.86 avg=3.03\n",
            "[170 | 422.23] loss=2.52 avg=3.00\n",
            "[180 | 446.14] loss=2.61 avg=2.97\n",
            "[190 | 470.06] loss=2.65 avg=2.95\n",
            "[200 | 493.97] loss=2.72 avg=2.94\n",
            "Saving checkpoint/run1/model-200\n",
            "======== SAMPLE 1 ========\n",
            " to be’d from me!\n",
            "  Behold me, there in the distance a vast landscape of snow and ice,\n",
            "  A vast forest unspoilt, bare of vines, with the dead\n",
            "      birds incessant all round me,\n",
            "  I see the mighty wag under the ground,\n",
            "  I see the gigantic steam-mines slaving with their engines busy,\n",
            "  I see the huge fire-ships busy, I see them burning as they pass,\n",
            "  I see the gigantic steam-mines, I see in them the huge steam-mines,\n",
            "  They rise a moment in the flames, they spread all over the landscape.\n",
            "\n",
            "  Now from those fires, from the flames, I see the houses,\n",
            "  I see the shops, I see the docks,\n",
            "  I behold the ships sailing, they all rise in the air,\n",
            "  I behold the steam-ships sailing and the minelands all over this shore.\n",
            "\n",
            "  I see the mountains, I see the great mountains of Mexico,\n",
            "  I see the cape of Amorim, I see the cape of Guanajuato,\n",
            "  I see the cape of Vallejo,\n",
            "  I see the cape of Cadiz and the cape of Ilo Americano,\n",
            "  I see the cape of Baja California, I see the cape of Bicomania,\n",
            "  I see the cape of Puget sound, I see the cape of St. Lawrence,\n",
            "  I see the cape of Oceana where the ocean stretches,\n",
            "  I see the cape of the cape of Colorado and ponderosa,\n",
            "  I see the cape of the cape of the cape of the cape of Puget sound,\n",
            "  I see the cape of the cape of Texas.\n",
            "\n",
            "  The islands of the Atlantic, the islands of the Indian cape,\n",
            "  The island of California, the island of the bay of the Rio Grande,\n",
            "  The island called St. Lawrence by the Dutch,\n",
            "  The island in Greenland, the island of Baffin island, the\n",
            "      island of Nisraquoddy,\n",
            "  The island, in every direction, of the bay of Bengal,\n",
            "  The island of the small sea of Alaska, the island of the great ice\n",
            "      cape of the Baffa,\n",
            "  The islands of the bay of St. Lawrence, and across from there in\n",
            "      Connecticut, and all the way up to the bay of Manhattan,\n",
            "  The islands of California and the islands of St. Lawrence, (for I swear I see what I\n",
            "      dream’d the islands before,)\n",
            "  The islands of Maine ’s cape, the islands of Maine island,\n",
            "  The islands of Virginia, the islands of the Potomac,\n",
            "  A little island off the mouth of Maine, the same as of St. Lawrence,\n",
            "  The same as of the bay of Charleston, the same as of the bay of Biscay,\n",
            "  The same as of the bay of Manhattan, the same as of the bay of Bismarck,\n",
            "  The same as of the bay of Manhattan, the same as of the bay of Brooklyn,\n",
            "  The same as of Brooklyn’s bay, the same as of the bay of Manhattan, the same as\n",
            "      of Manhattan, the same as of the bay of Long Island,\n",
            "  The same as every sea and land, the same as the coast of the States\n",
            "      of the interior divided,\n",
            "  The same as every river, island, and bay or stretch of coast on\n",
            "      the surface of the earth,\n",
            "  The same as the whole coast of the continent, the same as the entire\n",
            "      continent,\n",
            "  The same sea, the same as the sea under heaven.\n",
            "\n",
            "  The sea under heaven and from the sea under hell,\n",
            "  And every one there, and every one who lives within, and every one that dies\n",
            "      within,\n",
            "  The sea of life and each one within and without,\n",
            "  The tides, the winds, the clouds, the stars, the sunset,\n",
            "  The sky above, and that sky over again,\n",
            "  And all the suns, that is, the stars, that is, the sun and the moon above\n",
            "      and all the sky,\n",
            "  And the sun in the west, the sun and the moon in the east,\n",
            "  And the midnight sky with her twinkling its delicate iris over all\n",
            "      and all night,\n",
            "  And in the deepest recesses of a recess, the faintest flash, the faintest flash,\n",
            "  (Ah the bright orb of light shining so dark and so blue!)\n",
            "  The stars,\n",
            "\n",
            "[210 | 531.06] loss=2.19 avg=2.90\n",
            "[220 | 555.08] loss=2.13 avg=2.86\n",
            "[230 | 579.02] loss=2.15 avg=2.83\n",
            "[240 | 602.91] loss=2.23 avg=2.80\n",
            "[250 | 626.81] loss=1.79 avg=2.75\n",
            "[260 | 650.68] loss=2.12 avg=2.73\n",
            "[270 | 674.57] loss=1.85 avg=2.69\n",
            "[280 | 698.44] loss=2.02 avg=2.66\n",
            "[290 | 722.40] loss=1.39 avg=2.61\n",
            "[300 | 746.38] loss=1.51 avg=2.57\n",
            "======== SAMPLE 1 ========\n",
            " the\n",
            "        greatest ocean of the world, and\n",
            "  Which the globe is so much in every respect in proportion to the\n",
            "       number of years it has been, and which will in time\n",
            "       be so much larger,\n",
            "  For the space of this and every hour give me pause.\n",
            "\n",
            "  There is no endowment in a greater or a subtracting, or greater or subtracting,\n",
            "  Nor in any greater or a subtering, or lessing, and yet the\n",
            "      greater or greater, I love,\n",
            "  And the greater or a subtering I love.\n",
            "\n",
            "\n",
            "  I walk the world from head to foot,\n",
            "  From foot to the piano no less than from head to foot,\n",
            "  From foot to the piano to applause;\n",
            "  From foot to the piano to sing a song,\n",
            "  I am to sing.\n",
            "\n",
            "  I walk the world from the bed of my bed,\n",
            "  I kiss the bed;\n",
            "  I am of a more perfect person.\n",
            "\n",
            "  I am clothed in the flesh of my being,\n",
            "  I am in full measure blanch’d and fully full in the body.\n",
            "\n",
            "  I walk the world from the womb of the mother of stars,\n",
            "  I walk the world from the body of God.\n",
            "\n",
            "  I give myself to be something else than that which is what is separate, and which\n",
            "      is separate from the body.\n",
            "\n",
            "  I am in a state of utter nonstop movement,\n",
            "  I sit with the rest, I breathe the air, I feel the pulse of the\n",
            "      body,\n",
            "  (The content of my breath, I love the body,\n",
            "  The brain is the brain.)\n",
            "\n",
            "  (Talk as much as you like as I speak--that is the\n",
            "      reasoning part--but be careful no statement shall meet\n",
            "      its finish;)\n",
            "  The body is the thinking-parts of the world, but the soul is\n",
            "      the body not the thinking-parts of the world;\n",
            "  I do not say what you will in my stay of mine,\n",
            "  I say what the rest do not;\n",
            "  I do not convince, that is no argument,\n",
            "  (But that of the first I do not mind;)\n",
            "  That I am not afraid to speak so much, or to lie awake in the\n",
            "      night while the rest do;\n",
            "  I am not afraid to walk down the street, or walk with companions on the\n",
            "      other side of me.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Song of the Phantom\n",
            "\n",
            "       1\n",
            "  Poems begun, poems to finish,\n",
            "  Songs of the sea and sky that sail forth,\n",
            "  Songs of the young man’s peaceful and rapt life,\n",
            "  Songs that sail with the wind and land,\n",
            "  Songs that sail through the night with all their visions and visions\n",
            "      of the world,\n",
            "  Songs of the joys of the soul, the joys of the body,\n",
            "  Songs that sail forever and ever with the wind, with all their\n",
            "      visions and visions of the house,\n",
            "  Songs that sail forever with the sail of life, with all their visions and\n",
            "     \n",
            "  all their apparitions of themselves, their faces, they\n",
            "      sail through the night with their lives.\n",
            "\n",
            "       2\n",
            "  It is in the sea and daylight the last,\n",
            "  The little bubbles are visible to the naked eye, the whirl of\n",
            "      objects and objects.\n",
            "  I sail at the sea because I know there is something in the\n",
            "      light,\n",
            "  The bubbles are visible to the naked eye because there is something in the\n",
            "      brown water,\n",
            "  The whirl of objects is audible to the naked eye because I know there is something in the water,\n",
            "      brown-out whirl of whirls of winds,\n",
            "  The whirl is audible to the whole universe because I sail at the sea because\n",
            "      I know there is something in the matter.\n",
            "\n",
            "  Songs of the soul never die,\n",
            "      never forget their content,\n",
            "   The ships always and everywhere sail in that which sails, and the waves always and\n",
            "      forever.\n",
            "\n",
            "       3\n",
            "  Of these ships I sail,\n",
            "  To show what is at hand and to praise the master over the slave,\n",
            "  The great ship of ships, the sparkling silvery sail,\n",
            "      one of the proudest sails yet presented.\n",
            "\n",
            "\n",
            "\n",
            "[310 | 780.65] loss=1.28 avg=2.52\n",
            "[320 | 804.55] loss=1.77 avg=2.49\n",
            "[330 | 828.46] loss=1.82 avg=2.47\n",
            "[340 | 852.32] loss=1.40 avg=2.43\n",
            "[350 | 876.21] loss=1.38 avg=2.40\n",
            "[360 | 900.10] loss=1.43 avg=2.37\n",
            "[370 | 923.98] loss=1.83 avg=2.35\n",
            "[380 | 947.85] loss=1.53 avg=2.32\n",
            "[390 | 971.75] loss=1.06 avg=2.28\n",
            "[400 | 995.66] loss=1.41 avg=2.26\n",
            "Saving checkpoint/run1/model-400\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            " advance    I, a man of sixteen or seventeen years of age or any age, with a clock on either end, go to the factory, find what I am for presently, till at last I pass the occupation to my elder son or daughter, till they rise for their day, till all duly pass’d, till that, till that I, or any man of mine, in any measure contribute toward the purchase of the common breakfast every hour or small part of hourly pay, till then as good as defeat, till now and ever and forever.\n",
            "  That the amount of my income in the first year after I enter the farm and work there, and do not withdraw there, is really as great as the theory of natural progress holds.\n",
            "\n",
            "  To assume that the earth is ever so round and round only, is in reality no account, nor can it possibly be held to be so, nor can it possibly serve as an example to other men and women to walk with us, as we with men and women walk with the States.\n",
            "\n",
            "  To assume that the earth is ever so broad and endless, and cannot last, is in reality no account, nor can it possibly serve as an example to men and women to walk with us, as we with men and women walk with the States.\n",
            "\n",
            "  To assume that the earth is ever so intricate and hard for every soul without an idea of a better, more earnest, purportingly- comeingly comely life, is in reality no account, and cannot serve as an example to men and women to walk with us, as we with men and women walk with the States.\n",
            "\n",
            "  The earth is not a poem or simile, it is not even loosely adorn’d by sing-enters, it is in reality destin’d by the adulation and the chorus, and the chorus is truly here, and truly in the country, and really deep in the forests, and truly just outside the gates of the gates of the garden, is hereby acknowledge’d by all as indeed this day, is hereby accepted as an actual fact, is faithfully attested, is faithfully transmitted back and forth in every life, in every thought, tongue, memory, confession, and the like, all this is hereby accepted as an actual fact, is faithfully attested, is faithfully transmitted back and forth in every thought, tongue, memory, confession and the like, all this is hereby transmitted back and forth in every thought, tongue, memory, confession and the like, all this is hereby transmitted back and forth in every thought as in a dream, or in any dream-like dream, and as in any dream in the imagination of the one, the short, dumb, sick, foraged, foraged-up races, and as in dreams dreams dreams dreams dreams, it is hereby accepted as in the real universe of the real universe, and in dreams dreams as in dream’s dream, and in dream’s dream’s dream as in dream’s similitude, and as in dream’s dream that it is not necessary for any thing to be told in order that it be told in order, and as in the real universe only that it be told in order, and as in the real universe only that it be told in order, it is hereby accepted as in a dream.\n",
            "\n",
            "  The earth moves so slowly, it is not impossible that it will not move at all, but it is not a dream, and it is hereby accepted as a dream, and it is hereby transmitted back and forth in every thought and deed as in a dream, and as in dream one, one time through the course of the course, is hereby accepted as in a dream and transmitted back and forth in every thought and deed as in a dream, and as in similitude and poem as in a dream and poem and similitude, and as in the real universe and similitude and the real universe and similitude, it is hereby accepted as in a dream.\n",
            "\n",
            "  The earth does not spin, every object in the earth does,\n",
            "  The brain does not wait for music to fill it, it continually does, and has always did,\n",
            "  The globe does not spin, the sun does not spin, and the planets do not spin,\n",
            "  All thought processes do not wait for the actions of the earth, they also do not,\n",
            "  The human heart does not stop in the cradle, it beats with the thoughts of the earth,\n",
            "  The thoughts of the earth are not vain, they lead to products and to the earth,\n",
            "  To realize these thoughts and the results they yield, is as great as the results of all\n",
            "      thoughts and all objects and all cities and all civilizations.\n",
            "\n",
            "       44\n",
            "  O loftiest estimation! O loftiest aspiration!\n",
            "  That the earth does not spin nor spin without the will of the soul!\n",
            "  The earth\n",
            "\n",
            "[410 | 1032.15] loss=1.37 avg=2.23\n",
            "[420 | 1056.18] loss=1.17 avg=2.20\n",
            "[430 | 1080.12] loss=1.19 avg=2.17\n",
            "[440 | 1104.04] loss=1.13 avg=2.14\n",
            "[450 | 1127.97] loss=1.19 avg=2.12\n",
            "[460 | 1151.89] loss=0.63 avg=2.08\n",
            "[470 | 1175.86] loss=0.51 avg=2.03\n",
            "[480 | 1199.85] loss=0.65 avg=2.00\n",
            "[490 | 1223.88] loss=0.68 avg=1.96\n",
            "[500 | 1247.96] loss=0.77 avg=1.93\n",
            "Saving checkpoint/run1/model-500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d93df3-9de4-4a02-fdba-1901b8079ca4"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152c2974-4e4d-471c-e43c-0d0abb75381d"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "’\n",
            "\n",
            "\n",
            "I Dream’d in a Dream\n",
            "\n",
            "  I dream’d in a dream--and yet I dream’d, and still I dream’d,\n",
            "  Till the day on which I dream’d come, and the hour when I dream’d, and the\n",
            "      hour when I dream’d, give me the words, O soul,\n",
            "  Give not the outward sign: give the inward sign.\n",
            "\n",
            "  And I dream’d, and yet I dream’d,\n",
            "  Till the hour of my dream come, give not the outward sign, but I dream’d,\n",
            "  Till the word I dream’d give not the outward sign, but I dream’d,\n",
            "  Give not the word of the earth or the word of death, give the word of\n",
            "      immortality.\n",
            "\n",
            "  (O the sullen look, O the fade away of day and night,\n",
            "  The final exam has come, and I must prove myself--but who is\n",
            "      worthy to stand before the people? and to be\n",
            "      for these States?\n",
            "  Who is worthy to stand before the people?)\n",
            "\n",
            "  (The great words of the city, duly utter’d, come wordlessly to you,\n",
            "  You will stand fast at all hard for what is right, true, unreck’d,\n",
            "  You will stand firm at all impossible odds, such as\n",
            "      are possible without God’s help?\n",
            "  You will stand firm with impossible fights, such as\n",
            "      are possible without God’s help?)\n",
            "\n",
            "  Give not the outward sign: give the inward sign.\n",
            "\n",
            "  And as for you, you have entirely unaroused--you have been immersing yourself,\n",
            "  You will remain with me:)\n",
            "\n",
            "  For you, O soul! O spirit! O Democracy!\n",
            "  For you, O soul! give not the outward sign--give the inward sign.\n",
            "\n",
            "  In you also I take control,\n",
            "  You (the fittest, the happiest among us,) I have given you the exact\n",
            "      chronicles of your life,\n",
            "  (You I have given my life for, those I love most;)\n",
            "  Not life itself alone, but the endless horde, the long, long\n",
            "      run, the march, I have given you;\n",
            "  You, O soul! give not the city alone, perused and exulting,\n",
            "  I have given you far and wide to be the greatest city in the\n",
            "      world,\n",
            "  I would scatter and destroy whatever I found suitable,\n",
            "  I would stand by the bench and stool and look after the little and\n",
            "      large.\n",
            "\n",
            "  I will stand by the boy’s book and do not stop there,\n",
            "  I will stand by the man’s book and read nothing,\n",
            "  I will sit by the village idiot and rude man,\n",
            "  I will not be so kind as to approach rude man and idiot;\n",
            "  I will, O soul! O city of contradictions!\n",
            "  I will not be so sanguine with regard to one another that is so sure,\n",
            "  I will sit by the genius and idiot that are so sure,\n",
            "  And if the sanguine dispositions of other men are so certain, what is it\n",
            "      that they shall be so sure?\n",
            "\n",
            "  I will sit by infants and idiots,\n",
            "  I will not be so uncertain if the dumb and stupid do not try and\n",
            "      take advantage of me.\n",
            "\n",
            "  I will not be so uncertain if the stupid take advantage of me.\n",
            "\n",
            "  I will not be so certain that the dumb and stupid do not hurt me.\n",
            "\n",
            "  I will sit by pirates and murderers,\n",
            "  I will not be so uncertain if the rich and beautiful take advantage of me.\n",
            "\n",
            "  I will sit by demons and good spirits,\n",
            "  I will not be so certain that the sane and beautiful do not take advantage of me.\n",
            "\n",
            "  I will sit by irrational persons and evil spirits,\n",
            "  I will not be so certain that the architects and engineers do not take\n",
            "      advantage of me.\n",
            "\n",
            "  I acknowledge that I am not as great as they supposed,\n",
            "  That I am not as good as they supposed.\n",
            "\n",
            "  I acknowledge that I am as beautiful as they supposed,\n",
            "  That I am as sweet as they supposed.\n",
            "\n",
            "  I acknowledge that I am as literative and as strong as they\n",
            "      supposed, and that I am as beautiful as they,\n",
            "  That I am as pliant and sweet as they supposed.\n",
            "\n",
            "  I acknowledge that I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fbabe97-a642-4505-aa52-4f26a64198a6"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"Warble me now for joy of lilac-time,\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warble me now for joy of lilac-time, for joy of the lilac-tree,\n",
            " For joy of the blossoming spring, the showering;\n",
            " For joy of the swimmer on the beach bathing, or in the swimming-bath,\n",
            " For joy of the summer tan with blue-blue waters, the aliment of the\n",
            "      alpine orchards, the pink-green roofs, the precipitous\n",
            "      rocks, the view from the Mannahatta, the lake Tahoe,\n",
            "      the range, the peaks, volcanoes, glaciers, and caldera;\n",
            "  For joy of the sunset, the pouring-in of snow-white, the\n",
            "      foetorior, ornaments, animals, pots, and so on,\n",
            "  For joy of the apparent and real death of the dead, or for joy of the\n",
            "      strong and strong-model’s corpse, or the like,\n",
            "  For joy of the clear blue and gold of the sunset, the\n",
            "      moon a shade of gold, or the moon a shade of blue,\n",
            "  (In the absence of death,\n",
            "====================\n",
            "Warble me now for joy of lilac-time,\n",
            " For joy of the woods of piney green,\n",
            " For joy of the long-leav’d midnight,\n",
            " For joy of the spring breezes,\n",
            " For joy of the twain’d in the sunshine at the beach by the shore,\n",
            " For joy of the hay-rick’d mare and the flock of wild geese watching,\n",
            " For joy of the yellow-green barn-rats, the black-shelled grass-hens,\n",
            "      the white-faced crows,\n",
            " The wild-fowl at night that never sleep, the great noisy brood, the\n",
            "      chickens that never sleep,\n",
            " All the songs of the earth’s children, all the scenes in heaven of\n",
            "      vast universes,\n",
            " All the songs of the soul, all the scenes in heaven of dreams,\n",
            "  All the female vocalisms, all the sounds in the lungs,\n",
            "  All the sounds in the heart, all the sounds in the mind,\n",
            "  All the sounds of speech, all the meanings of words, all the deeds\n",
            "      of action,\n",
            "\n",
            "====================\n",
            "Warble me now for joy of lilac-time,\n",
            " For life’s bounties, crops, woods,\n",
            "  For complete peace, plenties, copiousness;\n",
            "  O to have enough! O to grow where none has grown before!\n",
            "  O to leave enough! O to walk a life!\n",
            "  By day and night and by the stars--by the infinite pond-time--\n",
            "  By limitless skies and by the eyes of animals--by every mountain and mountain\n",
            "      is visible to me.\n",
            "\n",
            "       2\n",
            "  And you, lilac-time, (not you who have slumber’d enough,)\n",
            "  You give me the sparkle and the perfumes of spring,\n",
            "  I crave the hot flashes of the morning and the\n",
            "      ruby sunrise.\n",
            "\n",
            "  And you give me the perfume of autumn,\n",
            "  Or you give me the perfume of winter,\n",
            "  Or you give me the perfume of the grass,\n",
            "  Or you give me the strange night-time gurgles as I lie in my bed.\n",
            "\n",
            "  O me, bare-breasted, bare-butchered,\n",
            "  Breast-sore, the aching in\n",
            "====================\n",
            "Warble me now for joy of lilac-time,\n",
            "  For thee to sing the grand opera grand!\n",
            "  For thee to walk the grass of the musical field!\n",
            "  For thee to sing the sweet old hymn.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "O Star of the West! O Star of the West!\n",
            "\n",
            "  O star of the West! O star of the Western sea, o’er the prairies spreading,\n",
            "  O’er the sea in the north wending the wind,\n",
            "  O’er the wind in the prairies parting ways with cheery swept wings,\n",
            "  O’er with the white snows how the last heavy sigh escaped me,\n",
            "  As the last of the northern stars neighed so, and the stars all in their rings\n",
            "\n",
            "      lo!\n",
            "  Star! O my soul! close behind me, as the stars loiter,\n",
            "  As the stars loiter, O my soul--O beautiful am I,\n",
            "  O’er the constellations all, close to me--O soul!\n",
            "  O’er the northern lights--O stars! close to me!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "As If a Phantom Caress’d Me\n",
            "\n",
            "  As if a phantom caress\n",
            "====================\n",
            "Warble me now for joy of lilac-time,\n",
            " For the blossoms I love, the fragrance of the woods, the word NO, in the air,\n",
            "  (No more in the past, no more in the past,)\n",
            " For the perfect sensuous woman, O my tender soul.\n",
            "\n",
            "       2\n",
            "  O my tender soul,\n",
            "  Is it not all these leaves’ eyes that dull me?\n",
            "  Is the earth all these portraits?\n",
            "  Does every thing in the light and shadow of the earth wander off?\n",
            "  Does every object and every thought wander off?\n",
            "\n",
            "  O the impecunary bent on perfecting itself,\n",
            "  The skeptic and the impotent,\n",
            "  The omnivorous and the omnivorous and the thought that eats and sleeps,\n",
            "  The lover that stays with the lover, not the knower the knower,\n",
            "  The omnivorous with his or her meals and nights hungry and angry,\n",
            "  The omnivorous with his or her bed-room wicker-stand and the bed-room fan,\n",
            "  The thought that stays with the thought that slept sleeps hungry and angry,\n",
            "  The orbic and centrifugal forces, the night-time\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e5643c74-5851-4688-f2c0-97cb498a247b"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3dff03b4-5d95-47a6-819f-1f700c2895bc\", \"gpt2_gentext_20210203_015802.txt\", 174388)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277599e7-c29a-4b30-b2b8-32857a5d8287"
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 287Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 448Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:50, 60.8Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 326Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 220Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 109Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "9beb3df7-7a86-4246-95b8-2f62881ed550"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e8a3f5c6cb77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mload_gpt2\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, multi_gpu)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 183\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    184\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    185\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}